# ğŸ“ utils/injest_json_chunks.py
import os
import json
from typing import List

# ìµœì‹  ë°©ì‹
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings


from langchain.schema import Document
from tiktoken import encoding_for_model

from dotenv import load_dotenv
load_dotenv()

import tiktoken

def count_tokens(text, model = 'text-embedding-3-small'):
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))

def split_documents_by_token_limit(docs: list[Document], max_tokens_per_batch: int = 200_000) -> list[list[Document]]:
    batches = []
    current_batch = []
    current_tokens = 0

    for doc in docs:
        tokens = count_tokens(doc.page_content)
        if current_tokens + tokens > max_tokens_per_batch:
            if current_batch:
                batches.append(current_batch)
            current_batch = [doc]
            current_tokens = tokens
        else:
            current_batch.append(doc)
            current_tokens += tokens

    if current_batch:
        batches.append(current_batch)

    return batches

# ì„¤ì •
STEP_PATHS = {
    "step2": "../../step2_results",
    "step4": "../../merged_step4_results",
    "step5": "../../step5_results"
}
CHROMA_PERSIST_DIR = "../vectorstore/step_json_chroma_db"
CHUNK_SIZE = 2000  # ê¸€ì ìˆ˜ ê¸°ì¤€ (token ê¸°ë°˜ìœ¼ë¡œ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´ ë”°ë¡œ ì„¤ì • í•„ìš”)
EMBEDDING_MODEL = "text-embedding-3-small"
openai_api_key = os.getenv("OPENAI_API_KEY")  # ë˜ëŠ” ì§ì ‘ í• ë‹¹

# í† í° ë¶„ë¦¬ê¸°
enc = encoding_for_model(EMBEDDING_MODEL)

def process_json_file(filepath: str, step: str) -> List[Document]:
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)

    docs = []

    # STEP2, 4, 5ì—ì„œ êµ¬ì¡°ê°€ ë‹¤ë¥´ë¯€ë¡œ ë¶„ê¸°
    if isinstance(data, list):  # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
        items = data
    elif isinstance(data, dict):  # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
        items = data.get("ë¬¸ì œì ëª©ë¡", []) if step == "step2" else data.get("ê°œì„ ë°©ì•ˆëª©ë¡", [])
    else:
        print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° í˜•ì‹: {filepath}")
        return []

    for item in items:
        content = json.dumps(item, ensure_ascii=False, indent=2)
        metadata = {"source": filepath, "step": step}
        docs.append(Document(page_content=content, metadata=metadata))

    return docs

def injest_all():
    all_docs = []

    for step, folder in STEP_PATHS.items():
        for fname in os.listdir(folder):
            if fname.endswith(".json"):
                fpath = os.path.join(folder, fname)
                docs = process_json_file(fpath, step)
                all_docs.extend(docs)

    print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(all_docs)}")
    batches = split_documents_by_token_limit(all_docs)

    for i, batch in enumerate(batches):
        print(f"âœ… ë°°ì¹˜ {i+1}/{len(batches)} - ë¬¸ì„œ ìˆ˜: {len(batch)}")
        db = Chroma.from_documents(
            documents=batch,
            embedding=OpenAIEmbeddings(openai_api_key=openai_api_key),
            persist_directory=CHROMA_PERSIST_DIR
        )

    print("ğŸ‰ ëª¨ë“  ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ.")

if __name__ == "__main__":
    injest_all()

