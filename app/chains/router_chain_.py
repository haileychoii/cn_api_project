# router_chain_.py

from langchain.chains import ConversationalRetrievalChain
from langchain_community.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.retrievers import EnsembleRetriever
from dotenv import load_dotenv
import os
from utils.prompt_templates import get_qa_prompt
from pathlib import Path
import re
import os

def load_conversational_chain():
    # âœ… API í‚¤ëŠ” app.pyì—ì„œ ì´ë¯¸ .envë¥¼ í†µí•´ ì„¤ì •ë˜ì—ˆë‹¤ê³  ê°€ì •

    # âœ… .env íŒŒì¼ì„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
    env_path = Path(__file__).resolve().parents[1] / ".env"
    print("ğŸ“ .env ê²½ë¡œ:", env_path)

    load_dotenv(dotenv_path=env_path)

    api_key = os.getenv("OPENAI_API_KEY")

    embedding = OpenAIEmbeddings(openai_api_key=api_key)

    chroma_vectorstore = Chroma(
        persist_directory="app/vectorstore/chroma_db",
        embedding_function=embedding
    )

    json_vectorstore = Chroma(
        persist_directory="app/vectorstore/step_json_chroma_db",
        embedding_function=embedding
    )

    chroma_retriever = chroma_vectorstore.as_retriever(search_kwargs={"k": 4})
    json_retriever = json_vectorstore.as_retriever(search_kwargs={"k": 4})

    combined_retriever = EnsembleRetriever(
        retrievers=[chroma_retriever, json_retriever],
        weights=[0.5, 0.5]
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

    llm = ChatOpenAI(
        temperature=0.3,
        model="gpt-4o-2024-05-13",
        openai_api_key=api_key
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=combined_retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": get_qa_prompt()},
        return_source_documents=True,
        output_key="answer"
    )

    return chain
